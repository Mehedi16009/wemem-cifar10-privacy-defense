# WEMEM-cifar10-privacy-defense
PyTorch implementation of WEMEM defense methods (RSW, RMR, SWMR) for CIFAR-10, with iterative pruning, memorization score estimation, LiRA membership inference attacks, and IEEE-ready result visualizations.

# WEMEM Privacy Defense on CIFAR-10

ðŸ”¹ Improvements Over Original WEMEM Paper

This implementation extends the original WEMEM: Defending Against Membership Inference Attacks on Iteratively Pruned Deep Neural Networks framework with several enhancements for better stability, privacyâ€“utility balance, and reproducibility.

âœ… 1. Adaptive Learning Rate Scheduling
	â€¢	Implemented Cosine Annealing with Warm Restarts to ensure smooth convergence during post-pruning fine-tuning.
	â€¢	Prevents overfitting and boosts recovery after each pruning stage.

âœ… 2. Early Stopping with Validation Monitoring
	â€¢	Added validation-based early stopping to reduce unnecessary training epochs.
	â€¢	Saves computation time while improving generalization.

âœ… 3. Dynamic Memorization Score Thresholding
	â€¢	Applied z-score normalization for memorization scores before thresholding.
	â€¢	Reduces noise and false positives in identifying high-memorization samples.

âœ… 4. Fine-Grained Iterative Pruning
	â€¢	Layer-wise L1 unstructured pruning with adaptive sparsity increments instead of fixed rates.
	â€¢	Achieves better accuracy retention while still removing redundant weights.

âœ… 5. Privacyâ€“Utility Re-Optimization
	â€¢	Tuned RSW, RMR, and SWMR defense parameters for lower Membership Inference Attack (MIA) success rates with minimal accuracy drop.

âœ… 6. Enhanced Evaluation Pipeline
	â€¢	Automatically plots Accuracy vs. MIA Success Rate after each pruning/defense stage.
	â€¢	Clear visual comparison between original model, pruned model, and defended model.

âœ… 7. Colab-Friendly & Reproducible
	â€¢	Fully runnable on Google Colab (T4/V100 GPUs) without code changes.
	â€¢	Switchable dataset option: CIFAR-10 / CIFAR-100 for broader testing.

â¸»

ðŸ“Š Example Results:
## ðŸ“ˆ Model Performance

### Confusion Matrix
![Confusion Matrix](assets/confusion_matrix.png)

### Training & Validation Accuracy
![Train and Validation Accuracy](assets/train_val_accuracy.png)

### Training & Validation Loss
![Train and Validation Loss](assets/train_val_loss.png)

â¸»

ðŸ“Š Planned Future Work:
	â€¢	LLM/VLM-powered interpretability to explain and visualize memorization patterns in pruned models.
	â€¢	Integration with automated hyperparameter tuning for optimal privacyâ€“utility tradeoffs.
